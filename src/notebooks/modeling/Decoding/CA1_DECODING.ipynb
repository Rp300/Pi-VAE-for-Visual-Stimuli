{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dec88b-b7de-47e8-b010-f1f662968970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8686cf66-7cc0-4106-84e9-dbd8be59c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU(s): ['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Check available GPU devices\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "# Print available GPUs\n",
    "available_gpus = get_available_gpus()\n",
    "if available_gpus:\n",
    "    print(\"Available GPU(s):\", available_gpus)\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759313fd-fb96-411e-8c6f-87b72423bca1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ddc8445-b691-4f8f-b417-f864f408859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import scipy.special as ssp\n",
    "import sys\n",
    "import scipy.io as sio\n",
    "sys.path.append(\"../code/\")\n",
    "from pi_vae import *\n",
    "from util import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## import plot packages\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib import ticker\n",
    "import seaborn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63749904-0ab7-4e8f-bcd7-1f84cba30dec",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f0990e-a210-4414-bf52-e9d827bca36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "ecephys_session_id = 771160300\n",
    "brain_region = 'VISp'\n",
    "\n",
    "directory_path = os.path.join(data_dir, f'session_{ecephys_session_id}')\n",
    "\n",
    "# Read spike count data pivoted by neurons and time bins\n",
    "spike_data_file = f'{brain_region}_spike_count_{ecephys_session_id}_pivot_p4.csv'\n",
    "spike_file_path = os.path.join(directory_path, spike_data_file)\n",
    "if not os.path.exists(spike_file_path):\n",
    "    raise FileNotFoundError(f\"File {spike_file_path} does not exist\")\n",
    "spike_count_pivot = pd.read_csv(spike_file_path)\n",
    "\n",
    "# Read behavior data\n",
    "behavior_data_file = f'NaturalMovie_Behavior_{ecephys_session_id}_normalized_p4.csv'\n",
    "behavior_file_path = os.path.join(directory_path, behavior_data_file)\n",
    "if not os.path.exists(behavior_file_path):\n",
    "    raise FileNotFoundError(f\"File {behavior_file_path} does not exist\")\n",
    "behavior_data_df = pd.read_csv(behavior_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c329b79-ce7a-46ee-83b4-427552fd35e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 82)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_count_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0a818b-7549-41eb-a805-991cd7585c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 82)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_count_pivot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d20bac-9135-440e-9375-010e2c7ccbd8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694f372f-a00e-46a7-93d0-447eb41f4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *\n",
    "\n",
    "selected_behavior_vars = ['frame']\n",
    "\n",
    "x_all, u_all = transform_data(spike_count_pivot, behavior_data_df, selected_behavior_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb4ce1-e7ed-4ed6-9e1b-f2b77a8b032a",
   "metadata": {},
   "source": [
    "### Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc56f8fc-39e5-49f3-ba4d-747ea611d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(frames_per_window, x_all, u_all):\n",
    "    # Check if frames_per_window is an odd number and divisor of the total frames\n",
    "    total_frames = x_all[0].shape[0]\n",
    "    if total_frames % frames_per_window != 0 or frames_per_window % 2 == 0:\n",
    "        raise ValueError(\"Frames per window must be an odd number and a divisor of the total number of frames.\")\n",
    "\n",
    "    # Calculate the index of the middle frame in each window\n",
    "    middle_index = frames_per_window // 2\n",
    "\n",
    "    # Summing frames within each window and take the spike counts\n",
    "    x_all_downsample = [np.sum(x_all[i].reshape(-1, frames_per_window, x_all[i].shape[-1]), axis=1) for i in range(len(x_all))]\n",
    "\n",
    "    # Ensure the new shape is as expected\n",
    "    print(\"New shape for each element:\", x_all_downsample[0].shape)\n",
    "\n",
    "    # Selecting the middle frame index from each window\n",
    "    u_all_downsample = [u_all[i][middle_index::frames_per_window] for i in range(len(u_all))]\n",
    "\n",
    "    # Ensure the new shape is as expected\n",
    "    print(\"New shape for each element in u:\", u_all_downsample[0].shape)\n",
    "\n",
    "    return np.array(x_all_downsample), np.array(u_all_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b95a53e-14a5-4196-b576-72f248e8f698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape for each element: (180, 82)\n",
      "New shape for each element in u: (180, 1)\n"
     ]
    }
   ],
   "source": [
    "frames_tick = 5\n",
    "x_all_downsample, u_all_downsample = downsample(frames_tick, x_all, u_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a58b5641-916d-442b-8f5a-2e45d6962f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 180, 82)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all_downsample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c0c5c-1187-4d2c-9a65-df6d606f1ba8",
   "metadata": {},
   "source": [
    "### Dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3cae72-3915-4941-b44a-af3315914430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  19380       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 82)           22296       encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 41,676\n",
      "Trainable params: 41,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output encoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to encoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n",
      "/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "# Define model?\n",
    "LATENT_DIM = 10\n",
    "MODEL = \"PiVAE\"\n",
    "\n",
    "np.random.seed(666);\n",
    "vae = vae_mdl(dim_x=x_all[0].shape[-1], \n",
    "                   dim_z=LATENT_DIM,\n",
    "                   dim_u=u_all[0].shape[-1], \n",
    "                   gen_nodes=60, n_blk=2, mdl='poisson', disc=False, learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19077d24-0c8d-4762-aaba-2cef15b7661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 22, 24, 26, 27, 28, 29, 30, 31, 33, 35, 36, 37, 38, 39,\n",
       "        40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
       "        57, 58, 59]),\n",
       " array([ 8, 21, 23, 25, 32, 34]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=666)\n",
    "fold = 0\n",
    "train, test=[], []\n",
    "for train_index, test_index in kf.split(x_all):\n",
    "    train, test = train_index, test_index\n",
    "    break\n",
    "\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fea06a-a54e-4a47-ab91-3401433a1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_valid = x_all[train], x_all[test]\n",
    "# u_train, u_valid = u_all[train], u_all[test]\n",
    "\n",
    "x_train, x_valid = x_all_downsample[train], x_all_downsample[test]\n",
    "u_train, u_valid = u_all_downsample[train], u_all_downsample[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb41120-fed2-4ce2-8f86-87a3f7c5f28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_chk_path = f'../results/F_{frames_tick}rsmpl_rat_{LATENT_DIM}d_666_{MODEL}_{selected_behavior_vars[0]}.h5' ##999, 777\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "# s_n = vae.fit_generator(custom_data_generator(x_train, u_train),\n",
    "#               steps_per_epoch=len(x_train), epochs=300, \n",
    "#               verbose=1,\n",
    "#               validation_data = custom_data_generator(x_valid, u_valid),\n",
    "#               validation_steps = len(x_valid), callbacks=[mcp]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7afa98f7-1083-4728-bda5-b67ab42d7579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14517d791160>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s_n.history['val_loss'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "205862d8-b588-491b-bb72-b5f0db2e6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_chk_path = '../results/F_rat_10d_666_VAE.h5'\n",
    "model_chk_path = f'../results/F_{frames_tick}rsmpl_rat_{LATENT_DIM}d_666_{MODEL}_{selected_behavior_vars[0]}.h5' ##999, 777\n",
    "vae.load_weights(model_chk_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4b0822-744f-4ab4-921c-621af98b875b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(900, 82), b.shape=(82, 60), m=900, n=60, k=82\n\t [[{{node encoder/dense_4/MatMul}}]]\n\t [[{{node encoder/dense_6/BiasAdd}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-96efc3d2c1bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs = vae.predict_generator(custom_data_generator(x_all, u_all),\n\u001b[0;32m----> 2\u001b[0;31m                                                 steps = len(x_all));\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# post_mean, post_log_var, z_sample,fire_rate, lam_mean, lam_log_var, z_mean, z_log_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## variance of each latent dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1578\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(900, 82), b.shape=(82, 60), m=900, n=60, k=82\n\t [[{{node encoder/dense_4/MatMul}}]]\n\t [[{{node encoder/dense_6/BiasAdd}}]]"
     ]
    }
   ],
   "source": [
    "outputs = vae.predict_generator(custom_data_generator(x_all, u_all),\n",
    "                                                steps = len(x_all));\n",
    "# post_mean, post_log_var, z_sample,fire_rate, lam_mean, lam_log_var, z_mean, z_log_var\n",
    "print(outputs[0].var(axis=0))  ## variance of each latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ce484-6988-405c-8f92-9d963e9528bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_all = [];\n",
    "for ii in range(len(x_all)):\n",
    "    z_pred_all.append(vae.predict([x_all[ii], u_all[ii]])[0][:,:]);\n",
    "z_pred_all = np.array(z_pred_all);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8335f-db67-4460-9fe2-b0c70e5db432",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = outputs[0].var(axis=0)\n",
    "# Sort the columns by variance in descending order and get the indices\n",
    "sorted_indices = np.argsort(variances)[::-1]\n",
    "\n",
    "# Create a new object containing the columns of outputs[0] sorted by variance\n",
    "sorted_outputs = outputs[0][:, sorted_indices]\n",
    "print(variances,'\\n')\n",
    "print(sorted_outputs.var(axis=0),'\\n',sorted_indices)\n",
    "\n",
    "u_values = behavior_data_df['frame'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2d106-8a11-49ed-893d-0ccbddadb745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b7a411c-8633-4cd8-9d55-739a3420d6a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a7e361-71cc-43bb-8481-9c21b55a4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def build_vae(dim_x, dim_u, latent_dim=10, gen_nodes=60, n_blk=2, mdl='poisson', disc=False, learning_rate=5e-4):\n",
    "    # Assuming vae_mdl is a function that returns a compiled VAE model\n",
    "    return vae_mdl(dim_x=dim_x, dim_z=latent_dim, dim_u=dim_u, gen_nodes=gen_nodes, n_blk=n_blk, mdl=mdl, disc=disc, learning_rate=learning_rate)\n",
    "\n",
    "def train_and_save_model(x_train, u_train, x_valid, u_valid, latent_dim, epochs, fold_number, random_seed):\n",
    "    # Build the VAE model\n",
    "    vae = build_vae(dim_x=x_train[0].shape[-1], dim_u=u_train[0].shape[-1], latent_dim=latent_dim)\n",
    "    \n",
    "    # Define the directory path for model checkpointing\n",
    "    # directory = f'../results/rat_{latent_dim}d_{random_seed}_NP_5_frames'\n",
    "    directory = f'../results/rat_{latent_dim}d_{random_seed}_NP_5_{brain_region}_{selected_behavior_vars[0]}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    model_chk_path = f'{directory}/model_fold_{fold_number}.h5'\n",
    "    \n",
    "    # Setup model checkpointing\n",
    "    mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    # Fit the model using fit_generator\n",
    "    s_n = vae.fit_generator(custom_data_generator(x_train, u_train),\n",
    "                            steps_per_epoch=len(x_train), epochs=epochs, \n",
    "                            verbose=1,\n",
    "                            validation_data=custom_data_generator(x_valid, u_valid),\n",
    "                            validation_steps=len(x_valid), callbacks=[mcp])\n",
    "    \n",
    "    # Clear the session to free up memory and prevent slowdowns\n",
    "    K.clear_session()\n",
    "\n",
    "def perform_k_fold_cv(x_all, u_all, latent_dim, epochs, random_seed, n_splits=5):\n",
    "    np.random.seed(random_seed);\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    fold = 0\n",
    "    kfold_indices = kf.split(x_all) # save and return for tuning curve use\n",
    "    \n",
    "    for train_index, test_index in kfold_indices:\n",
    "        x_train, x_valid = x_all[train_index], x_all[test_index]\n",
    "        u_train, u_valid = u_all[train_index], u_all[test_index]\n",
    "        \n",
    "        train_and_save_model(x_train, u_train, x_valid, u_valid, latent_dim, epochs, fold, random_seed)\n",
    "        fold += 1\n",
    "\n",
    "    return kfold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52738b8-d15f-426e-8d94-85d78faeaefb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initialize and Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5db688e8-669d-4d09-af73-bb490e7ef33e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_7[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 189.0878 - val_loss: 172.3425\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 116.6007 - val_loss: 155.3592\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 101.1849 - val_loss: 137.9073\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 95.6757 - val_loss: 133.8682\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 93.2245 - val_loss: 131.6370\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 90.5574 - val_loss: 129.1012\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.9933 - val_loss: 126.7521\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 85.5356 - val_loss: 124.1824\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 83.8047 - val_loss: 123.1753\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.7882 - val_loss: 121.7727\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.9364 - val_loss: 120.7233\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 78.5732 - val_loss: 120.0483\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.3294 - val_loss: 118.2765\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.0021 - val_loss: 115.8769\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.6218 - val_loss: 113.6737\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.9079 - val_loss: 111.8797\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.7188 - val_loss: 110.4545\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.2364 - val_loss: 109.5002\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.5877 - val_loss: 108.2902\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.5969 - val_loss: 108.4341\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4118 - val_loss: 108.0444\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.4488 - val_loss: 107.2842\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.8254 - val_loss: 107.2498\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.6286 - val_loss: 107.0291\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.8625 - val_loss: 106.5527\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.2641 - val_loss: 106.3888\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.5687 - val_loss: 106.5283\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2364 - val_loss: 106.6674\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4972 - val_loss: 106.2133\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.2873 - val_loss: 106.0427\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.7861 - val_loss: 105.5131\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.8568 - val_loss: 105.1325\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.4643 - val_loss: 104.5979\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8283 - val_loss: 104.5870\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8306 - val_loss: 105.3821\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6504 - val_loss: 104.9301\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.4902 - val_loss: 104.8430\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.0453 - val_loss: 105.3360\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9178 - val_loss: 105.3416\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.4793 - val_loss: 105.1006\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1888 - val_loss: 105.2161\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9592 - val_loss: 104.8224\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5434 - val_loss: 104.6188\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4134 - val_loss: 104.7230\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3159 - val_loss: 104.6302\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0093 - val_loss: 104.7896\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0924 - val_loss: 104.7039\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9881 - val_loss: 104.5857\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2857 - val_loss: 104.6212\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2560 - val_loss: 105.2243\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2363 - val_loss: 104.4575\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8913 - val_loss: 104.0076\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0165 - val_loss: 103.7991\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.3431 - val_loss: 104.4533\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9063 - val_loss: 103.7017\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6234 - val_loss: 103.8429\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3437 - val_loss: 103.9537\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.1363 - val_loss: 103.5169\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0659 - val_loss: 103.6282\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9238 - val_loss: 103.3164\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.2135 - val_loss: 103.5336\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5794 - val_loss: 103.5541\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.1019 - val_loss: 103.5533\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0883 - val_loss: 103.0836\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8016 - val_loss: 103.3895\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8785 - val_loss: 103.4097\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.6984 - val_loss: 103.1925\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8074 - val_loss: 103.7130\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9883 - val_loss: 103.4436\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9241 - val_loss: 104.4604\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8738 - val_loss: 104.5389\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3429 - val_loss: 104.7229\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.6518 - val_loss: 105.1565\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.3958 - val_loss: 104.1894\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.1660 - val_loss: 103.9630\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9533 - val_loss: 103.2637\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9515 - val_loss: 104.0985\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9321 - val_loss: 103.2960\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8011 - val_loss: 103.3590\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9048 - val_loss: 103.3804\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.1872 - val_loss: 103.1029\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9061 - val_loss: 103.8923\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.5738 - val_loss: 103.5862\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.3880 - val_loss: 103.1994\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1706 - val_loss: 102.7551\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1139 - val_loss: 102.4997\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1746 - val_loss: 102.9384\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1473 - val_loss: 103.4833\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8325 - val_loss: 103.5255\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6557 - val_loss: 104.4135\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6594 - val_loss: 104.2697\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.7495 - val_loss: 103.5146\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6297 - val_loss: 103.4543\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6091 - val_loss: 102.8890\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.7260 - val_loss: 102.8134\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.5435 - val_loss: 103.3193\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3968 - val_loss: 102.5706\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2076 - val_loss: 102.6732\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3691 - val_loss: 103.0695\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6117 - val_loss: 103.2878\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 190.0245 - val_loss: 174.3834\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 117.8317 - val_loss: 156.5899\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 101.8775 - val_loss: 141.8758\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 96.8601 - val_loss: 138.2081\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 94.1923 - val_loss: 135.7217\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 91.9106 - val_loss: 133.9347\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.7071 - val_loss: 132.2714\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.8999 - val_loss: 131.2509\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 86.3881 - val_loss: 130.2597\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 85.0301 - val_loss: 129.7189\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 83.5807 - val_loss: 128.6382\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.4269 - val_loss: 126.5678\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 80.1041 - val_loss: 126.0771\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.1229 - val_loss: 124.7107\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.7261 - val_loss: 121.9053\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.3402 - val_loss: 119.8326\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.9148 - val_loss: 118.5403\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.5152 - val_loss: 118.0704\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.1932 - val_loss: 117.1974\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.1622 - val_loss: 117.0787\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.2266 - val_loss: 116.7395\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.4189 - val_loss: 116.5071\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.5785 - val_loss: 116.1760\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.8993 - val_loss: 116.0721\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.2895 - val_loss: 115.5702\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.8200 - val_loss: 115.6414\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.2800 - val_loss: 115.2037\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.0123 - val_loss: 115.6007\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.5225 - val_loss: 115.7308\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.9447 - val_loss: 114.9946\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.6338 - val_loss: 114.9707\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.5064 - val_loss: 114.9206\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.0026 - val_loss: 115.3491\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.6048 - val_loss: 114.7880\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.3530 - val_loss: 114.3227\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.3075 - val_loss: 114.5736\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.3154 - val_loss: 114.7695\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2765 - val_loss: 113.7491\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0751 - val_loss: 113.7433\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.1169 - val_loss: 113.0242\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.1789 - val_loss: 113.0070\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0971 - val_loss: 113.1574\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.8873 - val_loss: 113.4006\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1686 - val_loss: 112.8770\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6537 - val_loss: 112.9173\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.4500 - val_loss: 112.7126\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.5458 - val_loss: 113.0071\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.0888 - val_loss: 112.6180\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7252 - val_loss: 112.8944\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.3884 - val_loss: 112.5131\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2527 - val_loss: 112.5816\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.0441 - val_loss: 112.5713\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9423 - val_loss: 112.5218\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7183 - val_loss: 112.5207\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.6682 - val_loss: 112.4508\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3985 - val_loss: 112.0165\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3964 - val_loss: 112.5822\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7729 - val_loss: 112.7790\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5789 - val_loss: 112.5727\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5332 - val_loss: 112.3047\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3469 - val_loss: 112.4519\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0474 - val_loss: 112.4151\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9751 - val_loss: 112.5261\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0909 - val_loss: 112.3283\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8219 - val_loss: 112.8263\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8824 - val_loss: 112.4803\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9097 - val_loss: 112.1937\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7288 - val_loss: 111.8012\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6636 - val_loss: 111.7861\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9080 - val_loss: 111.7069\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6466 - val_loss: 112.0584\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4340 - val_loss: 111.8591\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.1707 - val_loss: 111.9079\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2409 - val_loss: 112.6686\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3399 - val_loss: 112.1555\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2207 - val_loss: 111.9736\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1275 - val_loss: 112.3768\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4893 - val_loss: 111.5682\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6181 - val_loss: 112.0531\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8853 - val_loss: 111.7179\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.6527 - val_loss: 111.5604\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.5686 - val_loss: 111.4715\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.3559 - val_loss: 111.1773\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2556 - val_loss: 111.0285\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2738 - val_loss: 111.0455\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.1746 - val_loss: 110.8673\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9989 - val_loss: 110.4549\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0431 - val_loss: 110.1904\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0435 - val_loss: 110.7558\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0424 - val_loss: 110.3980\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9977 - val_loss: 110.5630\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6560 - val_loss: 110.9815\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6479 - val_loss: 110.7768\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7505 - val_loss: 110.5871\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6303 - val_loss: 110.7149\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4644 - val_loss: 110.6022\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7219 - val_loss: 110.5870\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8191 - val_loss: 110.3745\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6138 - val_loss: 111.3541\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4887 - val_loss: 111.3265\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 189.8670 - val_loss: 168.5333\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 118.3634 - val_loss: 152.2038\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 102.0971 - val_loss: 139.3904\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 96.9172 - val_loss: 136.1266\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 94.3085 - val_loss: 133.9036\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 92.1614 - val_loss: 131.7766\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.8354 - val_loss: 129.3325\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.8185 - val_loss: 128.2346\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 85.8731 - val_loss: 127.1364\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.1721 - val_loss: 125.9312\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 82.4623 - val_loss: 123.9948\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.4670 - val_loss: 122.3506\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 80.2230 - val_loss: 120.1781\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 78.2609 - val_loss: 117.1602\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.9657 - val_loss: 114.7491\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.6090 - val_loss: 114.1737\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.2851 - val_loss: 112.9628\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.3231 - val_loss: 113.3939\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.4033 - val_loss: 112.4349\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.2642 - val_loss: 111.7586\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.2804 - val_loss: 112.1080\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4569 - val_loss: 111.9673\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.6813 - val_loss: 111.2744\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.8576 - val_loss: 110.7103\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.2584 - val_loss: 110.3605\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.0514 - val_loss: 110.4173\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.7014 - val_loss: 109.6822\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.2184 - val_loss: 109.6607\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.5965 - val_loss: 109.6254\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.3621 - val_loss: 108.5118\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.5771 - val_loss: 108.6335\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.7572 - val_loss: 108.0560\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2032 - val_loss: 107.8071\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.8399 - val_loss: 107.7378\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.5692 - val_loss: 107.2196\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4337 - val_loss: 107.3587\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.2810 - val_loss: 107.2845\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.9324 - val_loss: 106.8446\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6168 - val_loss: 106.9029\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6084 - val_loss: 106.8535\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.8647 - val_loss: 106.6366\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.2126 - val_loss: 106.1873\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1453 - val_loss: 105.7406\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.0236 - val_loss: 105.8023\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8324 - val_loss: 105.9268\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6882 - val_loss: 106.0127\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5012 - val_loss: 106.0348\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.3741 - val_loss: 105.5824\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.0278 - val_loss: 105.6829\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8569 - val_loss: 105.3201\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9625 - val_loss: 106.0831\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7163 - val_loss: 105.1113\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5015 - val_loss: 105.2658\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.2731 - val_loss: 105.3267\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1612 - val_loss: 105.1307\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0341 - val_loss: 105.4458\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0279 - val_loss: 105.1161\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9820 - val_loss: 105.2230\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1388 - val_loss: 105.4966\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8859 - val_loss: 104.5298\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7838 - val_loss: 104.8034\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5298 - val_loss: 104.4972\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4885 - val_loss: 104.6397\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6741 - val_loss: 104.4769\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5190 - val_loss: 105.1099\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2386 - val_loss: 104.8694\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3226 - val_loss: 106.1516\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5871 - val_loss: 106.0259\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9165 - val_loss: 105.1498\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6190 - val_loss: 104.9758\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2976 - val_loss: 104.1699\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0444 - val_loss: 104.4981\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9036 - val_loss: 104.9427\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.6585 - val_loss: 104.2850\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.3766 - val_loss: 104.0442\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.3902 - val_loss: 103.3172\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.6471 - val_loss: 103.4631\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8168 - val_loss: 103.9257\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2263 - val_loss: 103.4133\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9371 - val_loss: 102.9747\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7713 - val_loss: 102.4759\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6671 - val_loss: 102.7169\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5532 - val_loss: 102.2335\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5377 - val_loss: 102.2057\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5428 - val_loss: 102.6500\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4690 - val_loss: 102.5892\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8458 - val_loss: 102.7749\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8137 - val_loss: 102.3541\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6854 - val_loss: 102.5405\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5462 - val_loss: 102.2217\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0757 - val_loss: 102.1320\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0539 - val_loss: 102.3606\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.1554 - val_loss: 102.5204\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8875 - val_loss: 102.3349\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5969 - val_loss: 101.9288\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5892 - val_loss: 102.1630\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.6987 - val_loss: 101.9941\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0062 - val_loss: 101.7054\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.6710 - val_loss: 102.2404\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.4257 - val_loss: 102.6258\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 189.5301 - val_loss: 128.6641\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 120.5768 - val_loss: 104.3241\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 104.3751 - val_loss: 93.1662\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 99.2587 - val_loss: 90.1640\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 96.8846 - val_loss: 88.4102\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 94.6616 - val_loss: 87.4508\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 92.4890 - val_loss: 86.4317\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 90.7600 - val_loss: 85.9559\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.2881 - val_loss: 84.0603\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.3303 - val_loss: 82.9299\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.2923 - val_loss: 80.0329\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.2210 - val_loss: 77.8710\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 78.8967 - val_loss: 77.1649\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.7237 - val_loss: 75.8288\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.1103 - val_loss: 75.4080\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.6492 - val_loss: 74.9992\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.8293 - val_loss: 74.8118\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.5301 - val_loss: 74.1737\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.2934 - val_loss: 73.8947\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.4210 - val_loss: 73.7853\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.0515 - val_loss: 74.3621\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.7600 - val_loss: 74.3140\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.9141 - val_loss: 74.0208\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.0740 - val_loss: 72.9499\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4240 - val_loss: 72.4917\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.1619 - val_loss: 72.3527\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.7190 - val_loss: 72.3503\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.3009 - val_loss: 72.1396\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.2282 - val_loss: 71.6770\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.9493 - val_loss: 70.9190\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.0075 - val_loss: 71.3921\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.3712 - val_loss: 71.3887\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.8963 - val_loss: 71.1523\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.4286 - val_loss: 70.5320\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.0582 - val_loss: 70.2288\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.9096 - val_loss: 70.4674\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.8559 - val_loss: 70.1184\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.3178 - val_loss: 69.8148\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.0643 - val_loss: 69.8792\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.0222 - val_loss: 69.5586\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.1297 - val_loss: 70.2018\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.5907 - val_loss: 69.4732\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.4427 - val_loss: 69.4034\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2211 - val_loss: 68.7645\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.1984 - val_loss: 68.6520\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2206 - val_loss: 68.3558\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2145 - val_loss: 68.4921\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.9934 - val_loss: 68.3691\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.7544 - val_loss: 68.7183\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.3227 - val_loss: 68.6549\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4810 - val_loss: 68.6649\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4877 - val_loss: 68.2862\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4237 - val_loss: 67.6694\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1086 - val_loss: 67.5824\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.8127 - val_loss: 67.3694\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.7934 - val_loss: 67.4209\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.7784 - val_loss: 67.8199\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.7096 - val_loss: 67.4261\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.9574 - val_loss: 67.7287\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.8645 - val_loss: 67.2989\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.4379 - val_loss: 66.8424\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1919 - val_loss: 66.9829\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8358 - val_loss: 66.6406\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.9258 - val_loss: 66.0579\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7766 - val_loss: 66.2355\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5494 - val_loss: 65.8717\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5919 - val_loss: 65.8225\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6604 - val_loss: 65.9504\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6888 - val_loss: 65.9466\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.4566 - val_loss: 65.8988\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5107 - val_loss: 65.6529\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5968 - val_loss: 65.8980\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5472 - val_loss: 65.6315\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8129 - val_loss: 66.1053\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2946 - val_loss: 65.4112\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1617 - val_loss: 64.6620\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1146 - val_loss: 65.1970\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2881 - val_loss: 64.9530\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8231 - val_loss: 64.8489\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3515 - val_loss: 64.8399\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9743 - val_loss: 64.6490\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0115 - val_loss: 64.9601\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7729 - val_loss: 64.4766\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7907 - val_loss: 64.1153\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7894 - val_loss: 64.3968\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6710 - val_loss: 64.2103\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0738 - val_loss: 64.6511\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7911 - val_loss: 64.4298\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5962 - val_loss: 64.8124\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6148 - val_loss: 64.4852\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5800 - val_loss: 65.0417\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6923 - val_loss: 64.5557\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7313 - val_loss: 64.8360\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8463 - val_loss: 64.7599\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1595 - val_loss: 64.7149\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3072 - val_loss: 65.0467\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0867 - val_loss: 65.2100\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5402 - val_loss: 64.5711\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3433 - val_loss: 64.6238\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.1612 - val_loss: 64.0975\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 187.7904 - val_loss: 171.6927\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 114.7375 - val_loss: 152.3440\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 99.2538 - val_loss: 138.5775\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 93.7987 - val_loss: 134.3918\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 91.4278 - val_loss: 133.0124\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.3130 - val_loss: 131.3788\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 86.8476 - val_loss: 130.5432\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 85.2954 - val_loss: 128.7633\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 83.7764 - val_loss: 126.7029\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 81.8145 - val_loss: 124.1774\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.6806 - val_loss: 123.1584\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 78.1769 - val_loss: 122.3730\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 77.4966 - val_loss: 121.7283\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 76.8689 - val_loss: 120.9364\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.5551 - val_loss: 119.5212\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 73.9166 - val_loss: 116.6327\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.1056 - val_loss: 115.8811\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 70.3252 - val_loss: 114.0278\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 69.1869 - val_loss: 113.7480\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 68.1339 - val_loss: 113.2185\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 67.1915 - val_loss: 112.3909\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 66.5101 - val_loss: 111.6029\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 66.0997 - val_loss: 111.3982\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 66.1768 - val_loss: 112.1008\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 65.1101 - val_loss: 112.0066\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 63.7973 - val_loss: 111.0564\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 62.9925 - val_loss: 110.4136\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 62.5389 - val_loss: 110.3496\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 62.1307 - val_loss: 109.5810\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 61.8065 - val_loss: 109.6479\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 61.4514 - val_loss: 109.4879\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 61.2924 - val_loss: 109.4347\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 60.9575 - val_loss: 109.3044\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6296 - val_loss: 109.3841\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.6037 - val_loss: 109.6004\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.3093 - val_loss: 109.1554\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1186 - val_loss: 109.3499\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1574 - val_loss: 108.5466\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7303 - val_loss: 108.6917\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.4777 - val_loss: 108.7669\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1690 - val_loss: 108.6395\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7691 - val_loss: 108.3784\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3844 - val_loss: 108.4414\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2430 - val_loss: 107.9919\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0218 - val_loss: 107.9576\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9711 - val_loss: 107.8310\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.7615 - val_loss: 107.9425\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.5900 - val_loss: 107.8049\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 57.2537 - val_loss: 107.6133\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.1898 - val_loss: 107.4111\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 57.0571 - val_loss: 107.4984\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 57.0262 - val_loss: 107.0908\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 56.9162 - val_loss: 107.3333\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7722 - val_loss: 107.8293\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 56.6295 - val_loss: 107.2247\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 56.3201 - val_loss: 107.2035\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 56.3700 - val_loss: 107.1627\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 56.2877 - val_loss: 106.8632\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 56.2609 - val_loss: 106.8293\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 56.0769 - val_loss: 107.0205\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9960 - val_loss: 107.4361\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 56.0615 - val_loss: 107.8876\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 55.6310 - val_loss: 107.3385\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 56.1084 - val_loss: 106.9792\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 1s 14ms/step - loss: 56.4578 - val_loss: 107.6611\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 55.9832 - val_loss: 107.9431\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5274 - val_loss: 107.6309\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.3962 - val_loss: 106.6532\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8818 - val_loss: 107.1715\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5882 - val_loss: 106.6369\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.4272 - val_loss: 106.7396\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.2095 - val_loss: 106.4712\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.1884 - val_loss: 106.6219\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9629 - val_loss: 106.5036\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8063 - val_loss: 107.1689\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.7388 - val_loss: 106.5550\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.4073 - val_loss: 106.2928\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.3129 - val_loss: 106.2248\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1076 - val_loss: 106.4076\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.3298 - val_loss: 106.4439\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1775 - val_loss: 106.3982\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.0741 - val_loss: 106.3213\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8653 - val_loss: 106.1156\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.7362 - val_loss: 106.2047\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.7941 - val_loss: 106.4908\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8318 - val_loss: 106.7266\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8954 - val_loss: 106.7554\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.4992 - val_loss: 106.3931\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.4495 - val_loss: 106.6177\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.4342 - val_loss: 105.9840\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6424 - val_loss: 105.7999\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3697 - val_loss: 105.9371\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2771 - val_loss: 105.7409\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.1700 - val_loss: 106.0281\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2008 - val_loss: 105.9216\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2056 - val_loss: 105.8251\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3466 - val_loss: 105.5190\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2436 - val_loss: 105.6933\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.5675 - val_loss: 106.1317\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.5661 - val_loss: 106.7362\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 188.3746 - val_loss: 173.5194\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 114.1900 - val_loss: 156.4553\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 97.9070 - val_loss: 144.2852\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 92.7719 - val_loss: 140.7006\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.8649 - val_loss: 138.5055\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.3352 - val_loss: 136.2865\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.9858 - val_loss: 134.6590\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 82.9277 - val_loss: 133.3814\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.2343 - val_loss: 133.6098\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.0982 - val_loss: 132.4685\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.4077 - val_loss: 132.4203\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.4506 - val_loss: 132.2476\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.9601 - val_loss: 129.3661\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.4729 - val_loss: 128.9788\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.6893 - val_loss: 124.5938\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.4045 - val_loss: 123.3296\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.6145 - val_loss: 123.0325\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.2231 - val_loss: 122.4072\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.1454 - val_loss: 121.8169\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.2077 - val_loss: 121.7819\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.5000 - val_loss: 121.1460\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.8067 - val_loss: 120.6951\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2057 - val_loss: 120.1590\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0308 - val_loss: 120.3866\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4831 - val_loss: 120.0128\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.0424 - val_loss: 119.6776\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.5698 - val_loss: 119.1442\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1690 - val_loss: 118.7280\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1119 - val_loss: 118.7499\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5305 - val_loss: 118.4918\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9162 - val_loss: 117.8626\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.6306 - val_loss: 117.4277\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.4912 - val_loss: 117.7577\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3735 - val_loss: 117.6203\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9608 - val_loss: 117.4514\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8195 - val_loss: 117.0347\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8860 - val_loss: 117.4942\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3597 - val_loss: 116.8299\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3780 - val_loss: 116.7726\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.1215 - val_loss: 116.6306\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.7868 - val_loss: 116.5621\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.4459 - val_loss: 117.1111\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.1819 - val_loss: 116.4327\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0758 - val_loss: 116.4821\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9087 - val_loss: 116.1785\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6978 - val_loss: 115.9145\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6681 - val_loss: 116.3138\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4766 - val_loss: 116.3287\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7150 - val_loss: 116.1897\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6074 - val_loss: 116.4729\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5400 - val_loss: 115.9335\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4508 - val_loss: 116.2756\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3072 - val_loss: 115.8620\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.2297 - val_loss: 115.9045\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4499 - val_loss: 115.8367\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4369 - val_loss: 115.8182\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8619 - val_loss: 115.8372\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9405 - val_loss: 116.7494\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.1279 - val_loss: 116.6602\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.2044 - val_loss: 116.0786\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7711 - val_loss: 115.8794\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0437 - val_loss: 116.3752\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9753 - val_loss: 116.1778\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.7241 - val_loss: 115.8928\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.0730 - val_loss: 115.8379\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8047 - val_loss: 115.7273\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.7790 - val_loss: 115.9212\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.5704 - val_loss: 115.4385\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.5999 - val_loss: 115.3613\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.3960 - val_loss: 115.5355\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.2265 - val_loss: 115.3226\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1397 - val_loss: 115.5531\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1738 - val_loss: 115.1889\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1272 - val_loss: 115.3129\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.0934 - val_loss: 115.3295\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.9912 - val_loss: 115.2668\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8807 - val_loss: 115.6412\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.5726 - val_loss: 115.3520\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.4900 - val_loss: 115.4586\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3321 - val_loss: 115.4186\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3241 - val_loss: 115.4012\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2589 - val_loss: 115.7890\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.1983 - val_loss: 115.5934\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2782 - val_loss: 115.2728\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.3204 - val_loss: 116.1696\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2537 - val_loss: 115.8649\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6236 - val_loss: 115.6774\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.4691 - val_loss: 115.6048\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.1950 - val_loss: 114.8369\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.6598 - val_loss: 115.0088\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.7823 - val_loss: 115.6018\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2828 - val_loss: 115.6000\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.6734 - val_loss: 115.7375\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.4252 - val_loss: 115.4504\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.0714 - val_loss: 115.1468\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.1409 - val_loss: 115.0629\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.9877 - val_loss: 115.1306\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.0413 - val_loss: 115.1449\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.1641 - val_loss: 115.0171\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.9424 - val_loss: 114.8629\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 190.4703 - val_loss: 160.1577\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 119.6851 - val_loss: 142.3157\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 103.1048 - val_loss: 130.3794\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 97.8794 - val_loss: 127.5037\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 95.1542 - val_loss: 125.4526\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 93.0736 - val_loss: 123.8150\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 90.3860 - val_loss: 122.4499\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 88.6322 - val_loss: 121.9141\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 86.9066 - val_loss: 121.3723\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.9910 - val_loss: 119.8675\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 82.7314 - val_loss: 117.8877\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 80.2252 - val_loss: 115.8403\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.7064 - val_loss: 114.0392\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.0497 - val_loss: 112.8758\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.5903 - val_loss: 112.6948\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.2766 - val_loss: 112.4248\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.4542 - val_loss: 112.9635\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.5846 - val_loss: 111.6098\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.2216 - val_loss: 110.6573\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.8864 - val_loss: 111.1280\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.9828 - val_loss: 110.7448\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.8639 - val_loss: 109.9310\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.8680 - val_loss: 109.9606\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.2539 - val_loss: 109.5329\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.5590 - val_loss: 109.0857\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.0426 - val_loss: 108.8134\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.6122 - val_loss: 108.3332\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.4040 - val_loss: 107.8221\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.1272 - val_loss: 107.7902\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.6939 - val_loss: 108.0041\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.5568 - val_loss: 107.5756\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.3591 - val_loss: 106.7370\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.8771 - val_loss: 106.8970\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.6655 - val_loss: 106.7343\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.4739 - val_loss: 105.9289\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.3395 - val_loss: 106.1400\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2569 - val_loss: 106.2893\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0163 - val_loss: 105.9015\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.5118 - val_loss: 105.6236\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.2505 - val_loss: 105.7044\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1194 - val_loss: 105.6829\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.9662 - val_loss: 105.5178\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.9560 - val_loss: 105.6845\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6397 - val_loss: 105.2667\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6744 - val_loss: 105.4635\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.3493 - val_loss: 105.3803\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 61.2982 - val_loss: 104.8076\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 61.2082 - val_loss: 104.6923\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6191 - val_loss: 104.7045\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1998 - val_loss: 104.6202\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.7085 - val_loss: 104.7523\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.5411 - val_loss: 105.1042\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6182 - val_loss: 106.5326\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.2288 - val_loss: 105.2914\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7882 - val_loss: 104.2823\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.9271 - val_loss: 104.3906\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8302 - val_loss: 103.9764\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9495 - val_loss: 103.4639\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5444 - val_loss: 104.0615\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3277 - val_loss: 103.6098\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.2328 - val_loss: 104.2496\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1942 - val_loss: 103.8619\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3326 - val_loss: 104.0514\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0491 - val_loss: 103.3626\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9380 - val_loss: 103.0903\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9338 - val_loss: 103.7478\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9806 - val_loss: 103.6957\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6285 - val_loss: 103.5753\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4792 - val_loss: 103.6421\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3703 - val_loss: 103.5679\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3361 - val_loss: 103.7583\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2672 - val_loss: 103.4984\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0542 - val_loss: 103.1205\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.1495 - val_loss: 103.2347\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0453 - val_loss: 103.2905\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0697 - val_loss: 103.0508\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9863 - val_loss: 103.3036\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0012 - val_loss: 102.9169\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2353 - val_loss: 103.1569\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0847 - val_loss: 103.4602\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0281 - val_loss: 103.1583\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.7628 - val_loss: 103.5583\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.6589 - val_loss: 102.8177\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.7771 - val_loss: 102.8473\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3912 - val_loss: 103.3391\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8341 - val_loss: 103.0518\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.4033 - val_loss: 102.5365\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0000 - val_loss: 102.7817\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7742 - val_loss: 103.0748\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0270 - val_loss: 102.7185\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0745 - val_loss: 102.4503\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8622 - val_loss: 102.7656\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6642 - val_loss: 102.9056\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6924 - val_loss: 102.7750\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5233 - val_loss: 102.5215\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5266 - val_loss: 102.5027\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5509 - val_loss: 102.6615\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4715 - val_loss: 102.5737\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3154 - val_loss: 102.9213\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.2015 - val_loss: 103.0143\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 191.6627 - val_loss: 108.6766\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 120.7292 - val_loss: 87.6991\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 104.0154 - val_loss: 77.7454\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 98.8570 - val_loss: 75.9950\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 96.0400 - val_loss: 75.5679\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 93.6648 - val_loss: 73.9408\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 91.5034 - val_loss: 72.6842\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.4174 - val_loss: 71.9005\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 88.0221 - val_loss: 71.7876\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 86.3372 - val_loss: 71.0968\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.6576 - val_loss: 71.2426\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 82.5520 - val_loss: 69.5038\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 80.2230 - val_loss: 68.9331\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 78.2996 - val_loss: 67.5867\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.5971 - val_loss: 67.7006\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.2494 - val_loss: 67.3488\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.8315 - val_loss: 66.8879\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.6117 - val_loss: 65.9899\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.7041 - val_loss: 65.4645\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.9046 - val_loss: 64.8365\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.2643 - val_loss: 63.9948\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.5331 - val_loss: 63.5433\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.0593 - val_loss: 63.5560\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4605 - val_loss: 63.4192\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.9279 - val_loss: 65.2065\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.2282 - val_loss: 62.7614\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.6927 - val_loss: 63.4048\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.3492 - val_loss: 60.9661\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.2062 - val_loss: 64.0535\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.9389 - val_loss: 61.7147\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.9206 - val_loss: 63.0671\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.5300 - val_loss: 60.5415\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.4471 - val_loss: 62.1282\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.0312 - val_loss: 59.4695\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.6604 - val_loss: 60.6130\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.1306 - val_loss: 58.9996\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.8660 - val_loss: 59.9926\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.6809 - val_loss: 59.1593\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.5055 - val_loss: 59.7824\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.4789 - val_loss: 58.1734\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2482 - val_loss: 59.9028\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0917 - val_loss: 58.3285\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.8745 - val_loss: 59.0145\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.8554 - val_loss: 57.7587\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.4195 - val_loss: 58.6641\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.3566 - val_loss: 57.9162\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1981 - val_loss: 58.0144\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1974 - val_loss: 58.3808\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.8354 - val_loss: 57.7280\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6281 - val_loss: 57.7133\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.4632 - val_loss: 57.6660\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.2690 - val_loss: 58.0345\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1494 - val_loss: 57.2844\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.1107 - val_loss: 57.7137\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.8481 - val_loss: 57.1723\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7277 - val_loss: 57.6584\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5985 - val_loss: 56.6473\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5014 - val_loss: 56.8869\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5288 - val_loss: 56.0617\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2532 - val_loss: 56.2997\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7758 - val_loss: 57.3092\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.5358 - val_loss: 57.2610\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.4582 - val_loss: 57.7443\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2934 - val_loss: 58.0012\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2394 - val_loss: 58.7775\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8456 - val_loss: 56.5512\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8156 - val_loss: 57.8588\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8221 - val_loss: 55.8837\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.0755 - val_loss: 56.5257\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8423 - val_loss: 55.1960\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7493 - val_loss: 55.9657\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8126 - val_loss: 54.9484\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1684 - val_loss: 54.9173\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6882 - val_loss: 54.9661\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6381 - val_loss: 54.9051\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.6346 - val_loss: 54.3956\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7119 - val_loss: 55.6215\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5832 - val_loss: 54.2119\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4868 - val_loss: 56.2162\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5648 - val_loss: 54.3059\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3754 - val_loss: 55.3023\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3112 - val_loss: 54.1934\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2776 - val_loss: 54.5889\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0581 - val_loss: 53.3270\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.1306 - val_loss: 55.3234\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9664 - val_loss: 54.2293\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9065 - val_loss: 55.4806\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3528 - val_loss: 54.1657\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9456 - val_loss: 54.5922\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2253 - val_loss: 54.3896\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.0090 - val_loss: 54.6887\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2229 - val_loss: 55.1043\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2025 - val_loss: 55.0073\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5717 - val_loss: 54.1308\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.4565 - val_loss: 54.1770\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.3251 - val_loss: 53.6597\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.7965 - val_loss: 53.3852\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.5057 - val_loss: 53.3634\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2925 - val_loss: 54.1591\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2057 - val_loss: 53.3232\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 187.2330 - val_loss: 123.4242\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 113.1190 - val_loss: 106.1106\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 97.0596 - val_loss: 96.3499\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 91.9299 - val_loss: 94.4681\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.2797 - val_loss: 94.3915\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.1596 - val_loss: 92.8027\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.9636 - val_loss: 91.9552\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 83.5145 - val_loss: 90.2526\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 82.0177 - val_loss: 89.2869\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.6844 - val_loss: 88.3846\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 77.6686 - val_loss: 87.8216\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.2823 - val_loss: 87.4678\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.5427 - val_loss: 87.1987\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.8938 - val_loss: 87.2329\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 74.0166 - val_loss: 86.4403\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.3175 - val_loss: 87.2949\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.4585 - val_loss: 86.5782\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.6353 - val_loss: 86.7135\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.7381 - val_loss: 85.7433\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.5565 - val_loss: 84.7726\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4554 - val_loss: 84.1653\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.0920 - val_loss: 83.3704\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.0585 - val_loss: 82.8915\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.3699 - val_loss: 82.9521\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.6815 - val_loss: 81.8497\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.1994 - val_loss: 81.9054\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.6284 - val_loss: 81.4240\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.2896 - val_loss: 81.2050\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.9216 - val_loss: 80.6011\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.5963 - val_loss: 79.2851\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.3417 - val_loss: 78.8996\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6582 - val_loss: 79.0836\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.0017 - val_loss: 78.8476\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.3257 - val_loss: 77.8419\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.7906 - val_loss: 78.1842\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5010 - val_loss: 77.9929\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.2755 - val_loss: 77.7684\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0576 - val_loss: 77.3048\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.9014 - val_loss: 77.6458\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1644 - val_loss: 78.0325\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5500 - val_loss: 77.1526\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2088 - val_loss: 78.1727\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.9159 - val_loss: 76.9628\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.5293 - val_loss: 76.3221\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.3814 - val_loss: 77.0911\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2273 - val_loss: 76.8541\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8314 - val_loss: 76.8486\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6413 - val_loss: 76.8144\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 56.4488 - val_loss: 75.8244\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.2372 - val_loss: 76.0395\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4682 - val_loss: 75.9457\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5370 - val_loss: 75.3564\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.1670 - val_loss: 75.9360\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8161 - val_loss: 74.4790\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8726 - val_loss: 74.8231\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.9307 - val_loss: 74.6082\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0588 - val_loss: 75.4286\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0225 - val_loss: 75.0550\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.2872 - val_loss: 74.1111\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8447 - val_loss: 73.6278\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8158 - val_loss: 73.4863\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.6442 - val_loss: 74.1414\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.4174 - val_loss: 73.6556\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.3524 - val_loss: 73.7085\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.1406 - val_loss: 73.6437\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.8950 - val_loss: 73.2206\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.5400 - val_loss: 73.3735\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2387 - val_loss: 72.4564\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.1230 - val_loss: 72.9366\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.0464 - val_loss: 72.2063\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.2352 - val_loss: 72.6607\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 53.1386 - val_loss: 72.5675\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.6883 - val_loss: 72.3846\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.4990 - val_loss: 71.4298\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.2672 - val_loss: 72.5060\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.1035 - val_loss: 72.2045\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 52.0028 - val_loss: 72.4857\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.7757 - val_loss: 72.1243\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.7065 - val_loss: 72.2286\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.5372 - val_loss: 71.3816\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.2511 - val_loss: 71.9610\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.2810 - val_loss: 72.2830\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.5080 - val_loss: 72.2921\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.7612 - val_loss: 72.1478\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.9433 - val_loss: 73.2778\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.7263 - val_loss: 72.6103\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.2263 - val_loss: 72.4799\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.9843 - val_loss: 71.9629\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.7421 - val_loss: 72.1899\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.8928 - val_loss: 72.1404\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.1538 - val_loss: 71.4997\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.4732 - val_loss: 70.9792\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.5617 - val_loss: 71.9052\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 51.1987 - val_loss: 70.6781\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.5991 - val_loss: 71.0978\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.3948 - val_loss: 70.5680\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.1704 - val_loss: 71.2455\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.0754 - val_loss: 71.5282\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 50.1477 - val_loss: 72.1327\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 49.9550 - val_loss: 71.7632\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  39420       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 249)          129017      encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 168,437\n",
      "Trainable params: 168,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 189.9756 - val_loss: 173.7517\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 118.4610 - val_loss: 158.1186\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 102.1941 - val_loss: 145.7621\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 96.8365 - val_loss: 142.6342\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 94.1370 - val_loss: 141.6139\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 91.9464 - val_loss: 139.2450\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 89.8875 - val_loss: 137.2248\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 87.7676 - val_loss: 136.0789\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 86.0593 - val_loss: 134.2982\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 84.1920 - val_loss: 133.3131\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 81.5491 - val_loss: 130.4294\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 79.0435 - val_loss: 127.0666\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 76.3087 - val_loss: 124.4104\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 75.0331 - val_loss: 122.6436\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 73.3383 - val_loss: 122.1004\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 72.2094 - val_loss: 121.4103\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 71.1930 - val_loss: 121.4802\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 70.3734 - val_loss: 121.1432\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 69.5040 - val_loss: 120.7366\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 68.4549 - val_loss: 119.9667\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 67.4504 - val_loss: 119.3271\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.7137 - val_loss: 119.2175\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 66.1163 - val_loss: 119.0532\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.5326 - val_loss: 118.7738\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 65.1464 - val_loss: 118.5522\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.6933 - val_loss: 118.1576\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.3590 - val_loss: 117.8605\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.1934 - val_loss: 117.5959\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 64.1350 - val_loss: 117.5979\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.5705 - val_loss: 117.5205\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.4662 - val_loss: 117.1011\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 63.0659 - val_loss: 116.7187\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.9918 - val_loss: 116.6108\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.6337 - val_loss: 116.8075\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.2805 - val_loss: 116.3545\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.2826 - val_loss: 116.5023\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 62.1370 - val_loss: 116.4249\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6701 - val_loss: 116.0895\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.5182 - val_loss: 116.1825\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.6028 - val_loss: 116.0804\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.3888 - val_loss: 115.9048\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 61.0532 - val_loss: 114.9447\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.9099 - val_loss: 114.7815\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.7213 - val_loss: 115.1171\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.4222 - val_loss: 114.5697\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2557 - val_loss: 115.5189\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1749 - val_loss: 114.9592\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.1618 - val_loss: 114.6374\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9804 - val_loss: 114.6266\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8689 - val_loss: 114.8024\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.8295 - val_loss: 113.8890\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 60.2998 - val_loss: 114.0594\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.9331 - val_loss: 113.2081\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.5781 - val_loss: 112.7010\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.4236 - val_loss: 113.4829\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.1106 - val_loss: 113.1471\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.3613 - val_loss: 113.2320\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 59.0262 - val_loss: 113.0813\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7311 - val_loss: 113.0079\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.8301 - val_loss: 112.1808\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.7348 - val_loss: 113.0864\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.5164 - val_loss: 112.8563\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 58.2238 - val_loss: 112.5897\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8079 - val_loss: 112.5494\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.8235 - val_loss: 113.4416\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.6254 - val_loss: 112.3511\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.5448 - val_loss: 112.4837\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2779 - val_loss: 112.0749\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.1751 - val_loss: 112.1607\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.1479 - val_loss: 112.1560\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.2284 - val_loss: 111.8970\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 57.0963 - val_loss: 111.9110\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8068 - val_loss: 111.7085\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.8207 - val_loss: 111.4596\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6807 - val_loss: 112.2163\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9051 - val_loss: 111.7763\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.9858 - val_loss: 111.8613\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.7113 - val_loss: 111.6818\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5790 - val_loss: 111.4551\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5002 - val_loss: 111.7419\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5445 - val_loss: 111.5904\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.4927 - val_loss: 111.9605\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3493 - val_loss: 112.1971\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.3184 - val_loss: 111.9552\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.6357 - val_loss: 111.8397\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.5338 - val_loss: 111.3912\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 56.0393 - val_loss: 111.5791\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.8111 - val_loss: 111.6942\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5757 - val_loss: 111.1575\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.5612 - val_loss: 111.5680\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.4842 - val_loss: 111.0952\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.2585 - val_loss: 111.1403\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.0997 - val_loss: 111.1546\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.0303 - val_loss: 111.2798\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.8036 - val_loss: 110.8556\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9337 - val_loss: 111.1634\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.1904 - val_loss: 111.0536\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.2594 - val_loss: 111.1555\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 55.1596 - val_loss: 111.4639\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 54.9577 - val_loss: 111.0630\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 666\n",
    "n_folds = 10\n",
    "\n",
    "kfold_indices = perform_k_fold_cv(x_all=x_all_downsample, u_all=u_all_downsample, latent_dim=10, epochs=100, random_seed=RANDOM_SEED, n_splits=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b774555-3d18-4be7-b51e-289d049dbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=666)\n",
    "kfold_indices = kf.split(x_all_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da5a5194-ea63-4861-94e2-f3926f666723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(s_n.history['val_loss'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8567b-95b3-4d34-801c-293de689920a",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235bdbc-a1ce-4d6f-99e7-75f9b1ca09eb",
   "metadata": {},
   "source": [
    "### Prepare random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e72e82-0fa5-46dd-8594-4332c4754eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_u_fake(x_test, frame_bins, nu_samples):\n",
    "    \"\"\"\n",
    "    Generate a synthetic u dataset (u_fake_np) based on provided frame bins and test data.\n",
    "\n",
    "    Parameters:\n",
    "    - x_test: array-like, the test dataset where each element corresponds to a session.\n",
    "    - frame_bins: array-like, unique frame indices to be used for generating u_fake.\n",
    "    - nu_samples: int, number of unique frame indices to sample.\n",
    "\n",
    "    Returns:\n",
    "    - np.array, synthetic u dataset (u_fake_np).\n",
    "    \"\"\"\n",
    "    np.random.seed(666)  # Seed for reproducibility\n",
    "    u_fake_np = []\n",
    "    \n",
    "    # Ensure that the number of samples does not exceed the length of frame_bins\n",
    "    nu_samples = min(nu_samples, len(frame_bins))\n",
    "    \n",
    "    for jj in range(nu_samples):\n",
    "        tmp_all_np = []\n",
    "        for ii in range(len(x_test)):\n",
    "            nn = x_test[ii].shape[0]\n",
    "            # Fill with the current frame bin value for all samples in this part of the dataset\n",
    "            tmp_np = np.full((nn, 1), frame_bins[jj])\n",
    "            tmp_all_np.append(tmp_np)\n",
    "        u_fake_np.append(np.array(tmp_all_np))\n",
    "    \n",
    "    return np.array(u_fake_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8832c0-df9e-4b2b-a7bf-8d1ee1d9312d",
   "metadata": {},
   "source": [
    "### Decode and compute metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20ca0fbb-b201-43f3-8cb0-b2c370c8f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import scipy.special as ssp\n",
    "# from data_processing import *\n",
    "\n",
    "# def build_vae(dim_x, dim_u, latent_dim=10, gen_nodes=60, n_blk=2, mdl='poisson', disc=False, learning_rate=5e-4):\n",
    "#     # Assuming vae_mdl is a function that returns a compiled VAE model\n",
    "#     return vae_mdl(dim_x=dim_x, dim_z=latent_dim, dim_u=dim_u, gen_nodes=gen_nodes, n_blk=n_blk, mdl=mdl, disc=disc, learning_rate=learning_rate)\n",
    "\n",
    "# def load_and_decode_vae(model, x_test, u_fake_np, model_path, n_sample=50, nu_samples=180):\n",
    "#     model.load_weights(model_path)\n",
    "#     pbar = tqdm(total=len(x_test) * len(u_fake_np), desc=\"Computing likelihoods for VAE\")\n",
    "#     lik_all_np = []\n",
    "#     for jj in range(len(x_test)):\n",
    "#         # print(f\"test {jj}\")\n",
    "#         y_test_batch = x_test[jj]\n",
    "#         lik_batch = compute_marginal_lik_single_batch(model, y_test_batch, u_fake_np, n_sample, pbar)\n",
    "#         lik_all_np.append(lik_batch)\n",
    "#     pbar.close()\n",
    "#     decode_use_np = np.array([(lik.reshape(nu_samples, -1, order=\"F\").argmax(axis=0)) for lik in lik_all_np])\n",
    "#     return decode_use_np\n",
    "\n",
    "# def load_and_decode_tc(x_train, x_test, frame_bins):\n",
    "#     tc = np.array(x_train).mean(axis=0) \n",
    "#     lik_tc = [np.array([np.exp((x_test[jj]*np.log(np.clip(tc[ii],1e-7,1e6)) - tc[ii]).sum(axis=-1)) for ii in range(tc.shape[0])]) for jj in range(len(x_test))];\n",
    "#     lik_tc_use = np.concatenate([np.log(lik_tc[jj].mean(axis=0)) - ssp.loggamma(x_test[jj]+1).sum(axis=-1) for jj in range(len(lik_tc))]);\n",
    "    \n",
    "#     decode_tc_use = np.array([lik_tc[jj].argmax(axis=0) for jj in range(len(lik_tc))]);\n",
    "#     # decode_tc_use_final = np.array([frame_bins[decode_tc_use[jj]] for jj in range(len(x_test))])\n",
    "#     return decode_tc_use\n",
    "    \n",
    "# def compute_error(decoded_indices, u_test, frame_bins):\n",
    "#     errors = [np.abs(frame_bins[decoded] - u[:, 0]) for decoded, u in zip(decoded_indices, u_test)]\n",
    "#     return np.median(np.concatenate(errors))\n",
    "\n",
    "# def run_decoding_for_fold(model, x_train, u_train, x_test, u_test, u_fake, model_path, frame_bins, fold_number, error_file_path, n_sample=50, bin_len=181):\n",
    "#     # Decoding\n",
    "#     # print(\"start to move\")\n",
    "#     decoded_indices_pivae = load_and_decode_vae(model, x_test, u_fake, model_path, n_sample)\n",
    "#     # print(\"moving\")\n",
    "#     decoded_indices_tc = load_and_decode_tc(x_train, x_test, frame_bins)\n",
    "\n",
    "#     # Median Error\n",
    "#     error_pivae = compute_error(decoded_indices_pivae, u_test, frame_bins)\n",
    "#     error_tc = compute_error(decoded_indices_tc, u_test, frame_bins)\n",
    "\n",
    "#     # Append errors to the unified error file\n",
    "#     # with open(error_file_path, 'a') as file:\n",
    "#     #     file.write(f\"{fold_number}, {error_pivae:.3f}, {error_tc:.3f}\\n\")\n",
    "#     print(f\"fold {fold_number}, Pi_vae error: {error_pivae:.3f}, TC error: {error_tc:.3f}\\n\")\n",
    "#     print(f\"Appended errors for fold {fold_number} to {error_file_path}\")\n",
    "from tqdm import tqdm\n",
    "import scipy.special as ssp\n",
    "from data_processing import *\n",
    "\n",
    "def load_and_decode_vae(model, x_test, u_fake_np, model_path, n_sample=50, nu_samples=180):\n",
    "    model.load_weights(model_path)\n",
    "    pbar = tqdm(total=len(x_test) * len(u_fake_np), desc=\"Computing likelihoods for VAE\")\n",
    "    lik_all_np = []\n",
    "    for jj in range(len(x_test)):\n",
    "        # print(f\"test {jj}\")\n",
    "        y_test_batch = x_test[jj]\n",
    "        lik_batch = compute_marginal_lik_single_batch(model, y_test_batch, u_fake_np, n_sample, pbar)\n",
    "        lik_all_np.append(lik_batch)\n",
    "    pbar.close()\n",
    "    decode_use_np = np.array([(lik.reshape(nu_samples, -1, order=\"F\").argmax(axis=0)) for lik in lik_all_np])\n",
    "    return decode_use_np\n",
    "\n",
    "def load_and_decode_tc(x_train, x_test, frame_bins):\n",
    "    tc = np.array(x_train).mean(axis=0) \n",
    "    lik_tc = [np.array([np.exp((x_test[jj]*np.log(np.clip(tc[ii],1e-7,1e6)) - tc[ii]).sum(axis=-1)) for ii in range(tc.shape[0])]) for jj in range(len(x_test))];\n",
    "    lik_tc_use = np.concatenate([np.log(lik_tc[jj].mean(axis=0)) - ssp.loggamma(x_test[jj]+1).sum(axis=-1) for jj in range(len(lik_tc))]);\n",
    "    \n",
    "    decode_tc_use = np.array([lik_tc[jj].argmax(axis=0) for jj in range(len(lik_tc))]);\n",
    "    # decode_tc_use_final = np.array([frame_bins[decode_tc_use[jj]] for jj in range(len(x_test))])\n",
    "    return decode_tc_use\n",
    "    \n",
    "def compute_error(decoded_indices, u_test, frame_bins):\n",
    "    errors = [np.abs(frame_bins[decoded] - u[:, 0]) for decoded, u in zip(decoded_indices, u_test)]\n",
    "    err_concat = np.concatenate(errors)\n",
    "    return np.median(err_concat), err_concat\n",
    "\n",
    "def run_decoding_for_fold(model, x_train, u_train, x_test, u_test, u_fake, model_path, frame_bins, fold_number, error_file_path, n_sample=50, bin_len=181):\n",
    "    # Decoding\n",
    "    # print(\"start to move\")\n",
    "    decoded_indices_pivae = load_and_decode_vae(model, x_test, u_fake, model_path, n_sample)\n",
    "    # print(\"moving\")\n",
    "    decoded_indices_tc = load_and_decode_tc(x_train, x_test, frame_bins)\n",
    "\n",
    "    # Median Error\n",
    "    error_pivae, error_pivae_concat = compute_error(decoded_indices_pivae, u_test, frame_bins)\n",
    "    error_tc, error_tc_concat = compute_error(decoded_indices_tc, u_test, frame_bins)\n",
    "\n",
    "    # Append errors to the unified error file\n",
    "    with open(error_file_path, 'a') as file:\n",
    "        file.write(f\"{fold_number}, {error_pivae:.3f}, {error_tc:.3f}\\n\")\n",
    "    print(f\"fold {fold_number}, Pi_vae error: {error_pivae:.3f}, TC error: {error_tc:.3f}\\n\")\n",
    "    print(f\"Appended errors for fold {fold_number} to {error_file_path}\")\n",
    "\n",
    "    return error_pivae_concat, error_tc_concat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9191179-d807-4b75-b5be-38f4ee00e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_path =  f'../results/F_{frames_tick}rsmpl_rat_{LATENT_DIM}d_666_{MODEL}_{selected_behavior_vars[0]}.h5'\n",
    "# def main(x_all, u_all, kfold_indices, latent_dim, random_seed):\n",
    "#     directory = f'../results/rat_{latent_dim}d_{random_seed}_NP_5_{brain_region}_{selected_behavior_vars[0]}'\n",
    "#     frame_bins = np.unique(u_all)\n",
    "#     # error_file_path = os.path.join(directory, 'model_errors_F.txt')\n",
    "\n",
    "#     # # Ensure the directory exists and initialize the error file\n",
    "#     # if not os.path.exists(directory):\n",
    "#     #     os.makedirs(directory)\n",
    "#     # with open(error_file_path, 'w') as file:\n",
    "#     #     file.write(\"Fold, Pi-VAE Error, TC Error\\n\")\n",
    "\n",
    "#     model = build_vae(dim_x=x_all[0].shape[-1], dim_u=u_all[0].shape[-1], latent_dim=latent_dim)\n",
    "#     # Process each fold\n",
    "#     for fold_number, (train_index, test_index) in enumerate(kfold_indices):\n",
    "#         # if fold_number < 8:\n",
    "#         #     continue\n",
    "#         print(fold_number)\n",
    "#         x_train, x_test = x_all[train_index], x_all[test_index]\n",
    "#         u_train, u_test = u_all[train_index], u_all[test_index]\n",
    "#         # x_test = x_test[:1] \n",
    "#         # u_test = u_test[:1]\n",
    "\n",
    "#         # Generate u_fake\n",
    "#         nu_samples = 180  # Define the number of unique frame indices to sample\n",
    "#         u_fake = generate_u_fake(x_test, frame_bins=frame_bins, nu_samples=nu_samples)\n",
    "\n",
    "#         # Decode and compute error for Pi-VAE & Tuning Curve\n",
    "#         # model_path = os.path.join(directory, f'model_fold_{fold_number}.h5')\n",
    "#         run_decoding_for_fold(model, x_train, u_train, x_test, u_test, u_fake, mdl_path, frame_bins, fold_number, None)\n",
    "\n",
    "def main(x_all, u_all, kfold_indices, latent_dim, random_seed):\n",
    "    # directory = f'../results/rat_{latent_dim}d_{random_seed}_NP_5_{brain_region}_{selected_behavior_vars[0]}'\n",
    "    directory = f'../results/rat_{latent_dim}d_{random_seed}_NP_5_frames'\n",
    "    frame_bins = np.unique(u_all)\n",
    "    # error_file_path = os.path.join(directory, 'model_errors.txt')\n",
    "\n",
    "    # # Ensure the directory exists and initialize the error file\n",
    "    # if not os.path.exists(directory):\n",
    "    #     os.makedirs(directory)\n",
    "    # with open(error_file_path, 'w') as file:\n",
    "    #     file.write(\"Fold, Pi-VAE Error, TC Error\\n\")\n",
    "\n",
    "    model = build_vae(dim_x=x_all[0].shape[-1], dim_u=u_all[0].shape[-1], latent_dim=latent_dim)\n",
    "    fold_error_lists = []\n",
    "    # Process each fold\n",
    "    for fold_number, (train_index, test_index) in enumerate(kfold_indices):\n",
    "        if fold_number > 2:\n",
    "            break\n",
    "        print(fold_number)\n",
    "        x_train, x_test = x_all[train_index], x_all[test_index]\n",
    "        u_train, u_test = u_all[train_index], u_all[test_index]\n",
    "        # x_test = x_test[:1] \n",
    "        # u_test = u_test[:1]\n",
    "\n",
    "        # Generate u_fake\n",
    "        nu_samples = 180  # Define the number of unique frame indices to sample\n",
    "        u_fake = generate_u_fake(x_test, frame_bins=frame_bins, nu_samples=nu_samples)\n",
    "\n",
    "        # Decode and compute error for Pi-VAE & Tuning Curve\n",
    "        model_path = os.path.join(directory, f'model_fold_{fold_number}.h5')\n",
    "        error_pivae_concat, error_tc_concat = run_decoding_for_fold(model, x_train, u_train, x_test, u_test, u_fake, model_path, frame_bins, fold_number, None)\n",
    "\n",
    "        fold_error_lists.append((error_pivae_concat, error_tc_concat))\n",
    "\n",
    "    return fold_error_lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed15eb75-9e1c-4228-80ca-9ab51eca4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=666)\n",
    "kfold_indices = kf.split(x_all_downsample)\n",
    "\n",
    "kfold_store = []\n",
    "for fold_number, (train_index, test_index) in enumerate(kfold_indices):\n",
    "    temp = (train_index, test_index)\n",
    "    kfold_store.append(temp)\n",
    "    # print(kfold_store[fold_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c684ec37-0219-4ec9-8e20-9b2718e1185b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_all_downsample[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "672d0cbd-8067-442d-a42e-6079d2acb0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output encoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to encoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n",
      "/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 10), (None,  19380       input_16[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 82)           22296       encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 41,676\n",
      "Trainable params: 41,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "0\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-67fa62189526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_all_downsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu_all_downsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-7b40465f2d8b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(x_all, u_all, kfold_indices, latent_dim, random_seed)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Decode and compute error for Pi-VAE & Tuning Curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'model_fold_{fold_number}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0merror_pivae_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_tc_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_decoding_for_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mfold_error_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_pivae_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_tc_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-16b1b831750c>\u001b[0m in \u001b[0;36mrun_decoding_for_fold\u001b[0;34m(model, x_train, u_train, x_test, u_test, u_fake, model_path, frame_bins, fold_number, error_file_path, n_sample, bin_len)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# print(\"start to move\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mdecoded_indices_pivae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_decode_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m# print(\"moving\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mdecoded_indices_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_decode_tc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-16b1b831750c>\u001b[0m in \u001b[0;36mload_and_decode_vae\u001b[0;34m(model, x_test, u_fake_np, model_path, n_sample, nu_samples)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_decode_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_fake_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_fake_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Computing likelihoods for VAE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mlik_all_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1235\u001b[0m                              ' elements.')\n\u001b[1;32m   1236\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2958\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m     \"\"\"\n\u001b[0;32m-> 2960\u001b[0;31m     \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2878\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 758\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "main(x_all=x_all_downsample, u_all=u_all_downsample, kfold_indices=kfold_store[:2], latent_dim=10, random_seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b192039f-7021-4757-9311-6f50435d0e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00222469, 0.00778643, 0.01334816, 0.0189099 , 0.02447164]),\n",
       " array([0.        , 0.16759777, 0.33519553, 0.5027933 , 0.67039106]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd_bins = np.linspace(0,30,180);\n",
    "hd_bins = np.unique(u_all_downsample)\n",
    "\n",
    "hdb[:5], hd_bins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49539819-f0a0-41d3-935a-4aacc6eae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tc_rd(y, hd, hd_bins): # compute empirical tunning curve of data\n",
    "    tuning_curve = np.zeros((len(hd_bins)-1, y.shape[1]));\n",
    "    for ii in range(len(hd_bins)-1):\n",
    "        data_pos = ((hd>=hd_bins[ii])*(hd<=hd_bins[ii+1]));\n",
    "        tuning_curve[ii,:] = y[data_pos,:].mean(axis=0);\n",
    "    return tuning_curve\n",
    "\n",
    "def get_tc(y, hd, bin_len=40): # compute empirical tunning curve of data\n",
    "    hd_bins = np.linspace(hd.min(),hd.max(),bin_len);\n",
    "    tuning_curve = np.zeros((len(hd_bins)-1, y.shape[1]));\n",
    "    for ii in range(len(hd_bins)-1):\n",
    "        data_pos = ((hd>=hd_bins[ii])*(hd<=hd_bins[ii+1]));\n",
    "        tuning_curve[ii,:] = y[data_pos,:].mean(axis=0);\n",
    "    return tuning_curve\n",
    "x_test = x_valid\n",
    "hd_bins = np.unique(u_all_downsample)\n",
    "tc = get_tc_rd(np.concatenate(x_train), np.concatenate(u_train)[:,0], hd_bins);\n",
    "\n",
    "lik_tc = [np.array([np.exp((x_test[jj]*np.log(np.clip(tc[ii],1e-7,1e6)) - tc[ii]).sum(axis=-1)) for ii in range(tc.shape[0])]) for jj in range(len(x_test))];\n",
    "\n",
    "lik_tc_use = np.concatenate([np.log(lik_tc[jj].mean(axis=0))-ssp.loggamma(x_test[jj]+1).sum(axis=-1) for jj in range(len(lik_tc))]);\n",
    "\n",
    "decode_tc_use = np.array([lik_tc[jj].argmax(axis=0) for jj in range(len(lik_tc))]);\n",
    "\n",
    "# hd_bins = np.linspace(0,1.6,100);\n",
    "\n",
    "decode_tc_use = np.array([hd_bins[decode_tc_use[jj]] for jj in range(len(x_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3366aa5-2ac7-4dcc-be70-06d1775f1792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020022246941045596"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = u_valid\n",
    "\n",
    "(np.median(np.concatenate([(np.abs(decode_tc_use[jj]-u_test[jj][:,0])) \n",
    "                for jj in range(len(u_test))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c9dc7-f856-4c1c-b3b0-2f7019f2256a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pivae_demo",
   "language": "python",
   "name": "pivae_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
